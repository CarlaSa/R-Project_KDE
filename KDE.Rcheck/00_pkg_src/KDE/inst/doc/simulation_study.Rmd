---
title: "Simulation Study to Compare Bandwidth Selection Methods"
author: "Charlotte Boys, Leon Patzig, Carla Sagebiel"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Simulation Study to Compare Bandwidth Selection Methods} 
  %\VignetteEngine{knitr::rmarkdown} 
  \usepackage[utf8]{inputenc}
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 4
)
```

```{r, include=FALSE}
devtools::load_all(".")
library(ggplot2)
library(tidyverse)
```

```{r setup}
library(KDE)
```

We follow the RKurs Vorlesung 15 and run a simulation study to compare our bandwidth simulation methods for various functions and kernels. 

## Computational time

Sadly, we note that the current implementation of the Goldenschluger-Lepski method is very slow across most kernels and functions, even for a small `maxEval` and minimal number of bandwidths for `h_prime` tested:
```{r}
pdf <- pdfs$Expo
data <- rejection_sample(1000, pdf)
kernel <- kernels$gaussian
```

```{r eval=FALSE}
system.time(
  h_opt_GL <- bandwidth_selection('GL', kernel, data, maxEval = 100, n_bandwidths=3)
  )
#> Warning in criterion_getter(Kernel, data, maxEval, lower = lower, upper =
#> upper, : You have chosen the lowest bandwidth smaller than 0.1. Taking value
#> `lower = 0.1`.
#>    user  system elapsed 
#>  53.356   1.418  55.186
h_opt_GL
#> [1] 0.1025965
```

This is due to multiple nested `sapply` loops, each performing integration. Given more time, this implementation would need to be vectorised, and integrate functions replaced with `Rcpp` implementations. 

Compare the speed to the PCO method with a higher value of maxEval:
```{r eval = FALSE}
system.time(
  h_opt_PCO <- bandwidth_selection('PCO', kernel, data, maxEval = 1e3)
  )
#>    user  system elapsed 
#>   2.853   0.123   2.996
h_opt_PCO
#> [1] 0.1468662
```
The Cross-validation method, with a higher value of maxEval (and so hopefully more accurate integrals) also becomes slow with some function and kernel combinations:
```{r eval=FALSE}
system.time(
  h_opt_CV <- bandwidth_selection('CV', kernel, data, maxEval = 1e3)
  )
#>    user  system elapsed 
#>  41.994   0.243  42.762
h_opt_CV
#> [1] 0.03334418
```
```{r include=FALSE}
h_opt_GL <- 0.1025965
h_opt_PCO <- 0.05380332
h_opt_CV <- 0.03334418

```

```{r}
KDE_GL <- get_kde(h_opt_GL, kernel, data)
KDE_PCO <- get_kde(h_opt_PCO, kernel, data)
KDE_CV <- get_kde(h_opt_CV, kernel, data)
```

```{r echo=FALSE}
ggplot(data.frame(x=c(-3, 3)), aes(x=x)) + 
    geom_path(aes(colour="red"), stat="function", fun=KDE_GL)+
    geom_path(aes(colour="blue"), stat="function", fun=KDE_PCO) +
    geom_path(aes(colour="green"), stat="function", fun=KDE_CV) +
    geom_path(aes(colour="black"), stat="function", fun=pdf) +
    scale_colour_identity("Function", guide="legend", 
                          labels = c("KDE_GL", "KDE_PCO", "KDE_CV", "True pdf"),
                          breaks = c("red", "blue", "green", "black")) 
```

We note without showing results here that our implementation of GL is uniformly slow across many function and kernel choices. The processing time for the CV method, on the other hand, approaches that of PCO for some choices of kernel and function. For reasons of computational time, we chose to omit the Goldenschluger-Lepski method from the current simulation study. However, it could be easily included if the processing time became tractable.

## Visual comparison

First, we adapt the `compare()` function seen in the lecture:

```{r eval=FALSE}
compare <- function(x_points=seq(-1, 1, len=600), 
                    real_functions=list(cauchy = pdfs$Cauchy), 
                    n=1000,  
                    try_kernels=list(gaussian=kernels$gaussian), 
                    methods=list(CV="CV"), 
                    reps=5){
  if(!is.list(try_kernels)| is.null(names(try_kernels))) stop("try_kernels must be named list")
  if(!is.list(real_functions)| is.null(names(real_functions))) stop("real_functions must be named list")
  if(!is.list(methods)| is.null(names(methods))) stop("methods must be named list")
  d <- length(real_functions)*length(try_kernels)*length(methods)
  res <- array(NA, dim=c(length(x_points), reps, d))
  cnt <- 1
  # iterate through all vectors of options
  for (f in real_functions){
    for (k in try_kernels){
      for (m in methods){
        cat("Working on combination: ", cnt, "of", d, "\n", "Method:", m, "\n")
        res[,, cnt] <- replicate(reps, {
          data <- rejection_sample(n,f)
          h <- bandwidth_selection(m, k, data, maxEval=1e3)
          KDE <- get_kde(h, k, data)
          KDE(x_points)
        })
        cnt <- cnt + 1
      }
    }
  }
  method_str <- paste(names(methods), collapse="_")
  func_str <- paste(names(real_functions), collapse="_")
  kernel_str <- paste(names(try_kernels), collapse="_")
  saveRDS(res, file=paste(kernel_str, method_str, func_str, reps, "res.RDS", sep="_"))
  res
}
```

```{r eval=FALSE, include=FALSE}
# helper function for plotting
plot_with_confidence_band <- function(x, Y, col) {
  rgb <- col2rgb(col)/255
  col_alpha <- rgb(rgb[1], rgb[2], rgb[3], 0.2)
  v <- apply(Y, 1, function(x) c(mean(x), sd(x)))
  lines(x, v[1, ], lwd=2, col=col)
  polygon(
    c(x, rev(x)),
    c(v[1, ]+v[2, ], rev(v[1, ]-v[2, ])),
    col = col_alpha,
    border = NA)
  lines(x, v[1, ]+v[2, ], lwd=1, col=col)
  lines(x, v[1, ]-v[2, ], lwd=1, col=col)
}
```

```{r eval=FALSE, include=FALSE}
plot_comparison <- function(f=list(cauchy = pdfs$Cauchy),
                            main=NA,
                            legend=NULL,
                            show_diff=TRUE,
                            reps=50,
                            ...) {
  x_new <- seq(-1, 1, len=600)
  res <- compare(x_new, f, reps=reps, ...)
  m <- dim(res)[3]
  del <- max(apply(res, c(1, 3), sd))
  f_new <- f(x_new)
  # plot results
  par(mar = c(0,0,1,0), ann=FALSE, xaxt="n", yaxt="n")
  plot(x_new, f_new, type="l", lwd=2, col=1, ylim = range(f_new) + del*c(-1, +1))
  grid()
  for (i in 1:m) plot_with_confidence_band(x_new, res[,,i], col=i+1)
  title(main = main)
  if (!is.null(legend)) legend("topright", legend=legend, lwd=2, col=2:(m+1))
  # create second plot (difference between true f and estimation)
  if (show_diff) {
    diff <- res - f_new
    plot(c(-1,1), c(0,0), type="l", lwd=2, col=1, ylim = c(-del, del))
    grid()
    for (i in 1:m) plot_with_confidence_band(x_new, diff[,,i], col=i+1)
  }
}
```

Using the helper function `plot_with_confidence_band()` with `plot_comparison()` as seen in the lecture, we plot a visual comparison of the Cross-Validation and PCO methods for Kernel Density Estimation. The real function is the Cauchy distribution, and we used the Gaussian kernel.

```{r eval=FALSE}
plot_comparison(methods=list(CV="CV", PCO="PCO"), 
                main = "n=1000, Cauchy distribution, Gaussian Kernel",
                legend = c("CV", "PCO"))
```

![KDEs vs. real function.](cauchy_gaussian_reps50_n1000.png)

![Difference between KDEs and real function.](diff_cauchy_gaussian_reps50_n1000.png)

## Numerical simulation study
We also adapt the `compare_ise()` function:
  
```{r eval=FALSE}
compare_ise <- function(real_functions = list(cauchy=pdfs$Cauchy), 
                        n=1000,
                        try_kernels=list(gaussian=kernels$gaussian),
                        methods=list(CV="CV"),
                        reps=3){
  x_points <- seq(-1, 1, length.out=600)
  time <- system.time(
    res <- compare(x_points, 
                   real_functions = real_functions, 
                   n = n, 
                   try_kernels = try_kernels, 
                   methods = methods,
                   reps = reps)
  )
  print(time)
  f_true <- sapply(real_functions, function(f) f(x_points))
  f_true <- f_true[,rep(seq_along(real_functions), each=length(try_kernels)*length(methods))]
  diff <- array(NA, dim=dim(res))
  for (j in 1:dim(res)[3]) diff[,,j] <- res[,,j] - f_true[,j]
  ise <- apply(diff^2, c(2, 3), mean)
  opts <- as_tibble(expand.grid(method=names(methods), kernel=names(try_kernels), func= names(real_functions)))
  return_obj <- add_column(opts[rep(1:nrow(opts), each=reps), ], ise=as.vector(ise))
  method_str <- paste(names(methods), collapse="_")
  func_str <- paste(names(real_functions), collapse="_")
  kernel_str <- paste(names(try_kernels), collapse="_")
  saveRDS(return_obj, file=paste(kernel_str, method_str, func_str, reps, "ISE.RDS", sep="_"))
  return_obj
}
```

Now, we are ready to run our numerical simulation using `compare_ise()`. We compare find Integrated Squared Errors (ISE) for the PCO method and the cross-validation method. We examine the ISE with the Epanechnikov and Gaussian distributions as kernels, and Cauchy and Gaussian distributions as real functions.

```{r eval=FALSE}
try_kernels <- list(epanechnikov=kernels$epanechnikov, gaussian=kernels$gaussian)
methods <- list(CV="CV", PCO="PCO")
real_functions <- list(cauchy=pdfs$Cauchy, normal = get_normal(0,1))
reps <- 100
data_ise <- compare_ise(real_functions, try_kernels=try_kernels, methods=methods, reps=reps)
```


```{r eval=FALSE, include=FALSE}
#NOTE: REMOVE THIS SECTION ONCE FILES ARE GENERATED
data_ise2 <- readRDS("epanechnikov_CV_PCO_cauchy_normal_100_ISE.RDS")
data_ise3 <- readRDS("gaussian_CV_PCO_normal_100_ISE.RDS")

data_ise2 %>%
  group_by(func, kernel, method) %>%
  summarise(mise = mean(ise), med_ise=median(ise), sd_ise = sd(ise), reps = n()) %>%
  ungroup() ->
  data_mise2

data_ise3 %>%
  group_by(func, kernel, method) %>%
  summarise(mise = mean(ise), med_ise=median(ise), sd_ise = sd(ise), reps = n()) %>%
  ungroup() ->
  data_mise3

saveRDS(data_mise2, file="epanechnikov_CV_PCO_cauchy_normal_100_MISE.RDS")
saveRDS(data_mise3, file="gaussian_CV_PCO_normal_100_MISE.RDS")
```

Following the lecture once again, we can now find find the Mean Integrated Square Error (MISE) for our chosen methods, functions and kernels:
```{r eval=FALSE}
data_ise %>%
  group_by(func, kernel, method) %>%
  summarise(mise = mean(ise), med_ise=median(ise), sd_ise = sd(ise), reps = n()) %>%
  ungroup() ->
  data_mise

kernel_str <- paste(names(try_kernels), collapse="_")
method_str <- paste(names(methods), collapse="_")
func_str <- paste(names(real_functions), collapse="_")

saveRDS(data_mise, file=paste(kernel_str, method_str, func_str, reps, "MISE.RDS", sep="_"))
```

```{r eval=TRUE, include=FALSE}
# Here's where we add all our objects together to create data_mise_full:
mise1 <- readRDS("gaussian_CV_PCO_cauchy_100_MISE.RDS")
mise2 <- readRDS("epanechnikov_CV_PCO_cauchy_normal_100_MISE.RDS")
mise3 <- readRDS("gaussian_CV_PCO_normal_100_MISE.RDS")
data_mise <- bind_rows(mise1, mise2, mise3)
data_mise
```

```{r}
data_mise
```
We can see that for the Gaussian kernel, Cross-validation tends to be more accurate than PCO. However, for the Epanechnikov kernel, the difference is barely noticeable.


We can make our results more readable by looking specifically at the effect of the chosen kernel on the MISE of the two methods:
```{r}
data_mise %>%
  select(method, kernel, mise) %>%
  pivot_wider(names_from=kernel, values_from=mise, values_fn=mean) -> kernels_mise
kernels_mise
```
As expected, we see that on average across the real functions, the Kernel doesn't have much of an effect.

Let us look more closely at the effect of the real function on the MISE of the two methods:
```{r}
data_mise %>%
  select(method, func, mise) %>%
  pivot_wider(names_from = func, values_from=mise, values_fn=mean) -> functions_mise
functions_mise
```
We see that both methods work better for estimating the Gaussian distribution than the Cauchy distribution. In both cases, the Cross-validation method is slightly more accurate. 

Unfortunately we were only able to examine a limited set of functions and kernels due to computational time restrictions. Nevertheless, through the simulation study we have found that the simplest of all bandwidth-selection methods, Cross-validation, is in fact overall the most accurate for the functions tested, although the only slightly. More simulations are needed to assess the comparative speed of the two methods across multiple functions and kernels. 